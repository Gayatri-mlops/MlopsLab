**Data Labeling Labs — Snorkel, Weak Supervision, and Spam Classification**

Lab 6: Data Labeling Labs — Snorkel, Weak Supervision, and Spam Classification
Student : Gayatri Nair Date : November 2025

This project explores how Snorkel can be used to:
Build weak supervision pipelines
Improve labeling quality
Perform data augmentation
Analyze model behavior on specific data slices
Train a modern transformer model (DistilBERT) using weak labels

Everything is demonstrated using a real-world task: spam comment detection on YouTube comments.

Training ML Models:
  Labeled data provides the examples ML models need to learn patterns. Without labels, models cannot learn what is spam vs. not spam.

Evaluation (Ground Truth):
  Labeled datasets help measure how accurate a model is and highlight where it fails.

Better Insights:
  When data is categorized properly, it becomes easier to understand trends, behavior, and decision-making patterns.

Weak Supervision with Snorkel:
  Snorkel lets us replace slow manual labeling with programmatically generated labels using:

Labeling functions
Heuristics
Keyword rules
Distant supervision
Probabilistic label models

By combining multiple noisy signals, Snorkel produces strong, denoised labels that can train powerful models quickly.

Project Structure

```
Data_Labeling_Labs/
├── data/
│   ├── Youtube01-Psy.csv
│   ├── Youtube02-KatyPerry.csv
│   ├── Youtube03-LMFAO.csv
│   ├── Youtube04-Eminem.csv
│   ├── Youtube05-Shakira.csv
│   └── augmented_train.csv
│
├── models/
│   ├── distilbert_weak_spam/
│   │   ├── checkpoint-180/
│   │   │   ├── config.json
│   │   │   ├── model.safetensors
│   │   │   ├── optimizer.pt
│   │   │   ├── rng_state.pth
│   │   │   ├── scheduler.pt
│   │   │   ├── trainer_state.json
│   │   │   └── training_args.bin
│   │   └── runs/
│   │       ├── Nov29_19-34-42_G/
│   │       ├── Nov29_20-11-46_G/
│   │       └── Nov29_20-47-30_G/
│   │
│   ├── 04_distilbert_output.txt
│   ├── data_augmentation_metrics.txt
│   ├── slice_metrics.csv
│   ├── slicing_overall_metrics.txt
│   └── weak_labels.npy
│
├── notebooks/
│   ├── 01_spam_labeling.py
│   ├── 02_spam_data_augmentation.py
│   ├── 03_spam_data_slicing.py
│   └── 04_spam_distilbert_weak_supervision.py
│
└── src/
    ├── __init__.py
    └── spam_data_utils.py

```

Tutorial Breakdown

A. Notebooks

1. Spam Labeling with Weak Supervision (`01_spam_labeling.ipynb`)

This notebook introduces:

* Writing Labeling Functions (LFs)
* Applying LFs with Snorkel
* Training Snorkel’s LabelModel to denoise weak labels
* Training a bag-of-words classifier using the generated labels
* Saving:

  * `weak_labels.npy`
  * vectorizer
  * weak label classifier

This forms the foundation for later tutorials.


2. Data Augmentation (`02_spam_data_augmentation.ipynb`)

This tutorial shows how to improve model performance using:

* Transformation Functions (TFs)
* Text modifications (repeat, lowercase, add exclamation marks, etc.)
* Applying transformations with Snorkel’s `PandasTFApplier`
* Training models with both original + augmented data

This helps balance datasets and improve robustness.


3. Data Slicing for Model Debugging (`03_spam_data_slicing.ipynb`)

This notebook focuses on slice-based evaluation where we check how well the model performs on specific subsets of data:

Example slices:

* Short comments
* Comments containing “please” / “plz”
* Comments with “check ... out” patterns

The tutorial saves:

* `models/slice_metrics.csv`
* `models/slicing_overall_metrics.txt`

Slices reveal hidden weaknesses that the overall accuracy might hide.


4. Weak Supervision + DistilBERT (`04_spam_distilbert_weak_supervision.py`)

This script trains a DistilBERT transformer using only weak labels from Snorkel.

It includes:

* Loading weak labels
* Tokenizing using DistilBERT tokenizer
* Creating a custom dataset class
* Fine-tuning DistilBERT on weak labels
* Evaluating on real test labels

This demonstrates how powerful modern models can learn even from noisy labels when combined with Snorkel.


B. Dataset

We use five YouTube comment datasets:

```
data/
 ├── Youtube01-Psy.csv
 ├── Youtube02-KatyPerry.csv
 ├── Youtube03-LMFAO.csv
 ├── Youtube04-Eminem.csv
 └── Youtube05-Shakira.csv
```

Each file contains:

 content` → comment text
`class` → spam (1) or not spam (0)


C. Data

Raw and processed datasets for YouTube spam detection:

Original YouTube comment datasets

augmented_train.csv → Snorkel TF-generated augmented training data.


D. Models

All model outputs and metrics:

distilbert_weak_spam/ – HuggingFace DistilBERT fine-tuned on weak labels

checkpoint-180/ – trained weights & training state

E. Runs

TensorBoard logs for training

weak_labels.npy – Weak labels generated by Snorkel LabelModel

slice_metrics.csv – Per-slice F1 metrics

slicing_overall_metrics.txt – Global slicing report

data_augmentation_metrics.txt – Accuracy after augmentation

04_distilbert_output.txt – Final DistilBERT training summary


These scripts run end-to-end using:

Snorkel LFs

Snorkel TFApplier

Snorkel Slice functions

HuggingFace Trainer


F. src

Utility code:

spam_data_utils.py – Dataset loader, preprocessing helpers

__init__.py – Package initializer


How to Run

Install all dependencies:

```bash
pip install -r requirements.txt
pip install transformers
```

Run notebooks:

```bash
jupyter notebook
```

Run DistilBERT script:

```bash
python notebooks/04_spam_distilbert_weak_supervision.py
```

Conclusion

This project gives a complete understanding of:

How weak supervision works
How Snorkel improves labeling without manual effort
How to debug ML slices
How to use augmented data
How to train transformer models using weak labels

It demonstrates how real-world ML systems combine programmatic labeling, model debugging, augmentation, and modern NLP models to build reliable pipelines.
